[{"authors":null,"categories":null,"content":"Mathieu Fontaine is a Postdoctoral Researcher in RIKEN institute and Guest at Kyoto University. His interest are mainly audio source separation including speech enhancement, source localization and music source separation using probablistic models as heavy-tailed models.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://matfontaine.github.io/author/mathieu-fontaine/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/mathieu-fontaine/","section":"authors","summary":"Mathieu Fontaine is a Postdoctoral Researcher in RIKEN institute and Guest at Kyoto University. His interest are mainly audio source separation including speech enhancement, source localization and music source separation using probablistic models as heavy-tailed models.","tags":null,"title":"Mathieu Fontaine","type":"authors"},{"authors":["Pedro Morgado","Nuno Nvasconcelos","Timothy Langlois","Oliver Wang"],"categories":[],"content":"","date":1608716006,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608716006,"objectID":"768f74ed21db308f2a3e23f6a0fbf175","permalink":"https://matfontaine.github.io/publication/morgado-self-supervised-nodate/","publishdate":"2020-12-23T09:33:26.136316Z","relpermalink":"/publication/morgado-self-supervised-nodate/","section":"publication","summary":"We introduce an approach to convert mono audio recorded by a 360◦ video camera into spatial audio, a representation of the distribution of sound over the full viewing sphere. Spatial audio is an important component of immersive 360◦ video viewing, but spatial audio microphones are still rare in current 360◦ video production. Our system consists of end-to-end trainable neural networks that separate individual sound sources and localize them on the viewing sphere, conditioned on multi-modal analysis of audio and 360◦ video frames. We introduce several datasets, including one ﬁlmed ourselves, and one collected in-the-wild from YouTube, consisting of 360◦ videos uploaded with spatial audio. During training, ground-truth spatial audio serves as self-supervision and a mixed down mono track forms the input to our network. Using our approach, we show that it is possible to infer the spatial location of sound sources based only on 360◦ video and a mono audio track.","tags":[],"title":"Self-Supervised Generation of Spatial Audio for 360° Video","type":"publication"},{"authors":["Rui Shu","Hung H Bui","Shengjia Zhao","Mykel J Kochenderfer","Stefano Ermon"],"categories":[],"content":"","date":1608716005,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608716006,"objectID":"6c13263291455ecaf37ac7f048b55a51","permalink":"https://matfontaine.github.io/publication/shu-amortized-nodate/","publishdate":"2020-12-23T09:33:25.971465Z","relpermalink":"/publication/shu-amortized-nodate/","section":"publication","summary":"The variational autoencoder (VAE) is a popular model for density estimation and representation learning. Canonically, the variational principle suggests to prefer an expressive inference model so that the variational approximation is accurate. However, it is often overlooked that an overly-expressive inference model can be detrimental to the test set performance of both the amortized posterior approximator and, more importantly, the generative density estimator. In this paper, we leverage the fact that VAEs rely on amortized inference and propose techniques for amortized inference regularization (AIR) that control the smoothness of the inference model. We demonstrate that, by applying AIR, it is possible to improve VAE generalization on both inference and generative performance. Our paper challenges the belief that amortized inference is simply a mechanism for approximating maximum likelihood training and illustrates that regularization of the amortization family provides a new direction for understanding and improving generalization in VAEs.","tags":[],"title":"Amortized Inference Regularization","type":"publication"},{"authors":["Kazuyoshi Yoshii","Kouhei Sekiguchi","Yoshiaki Bando","Mathieu Fontaine","Aditya Arie Nugraha"],"categories":[],"content":"","date":1608716005,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608716005,"objectID":"a0a121ec3030898a5b14304fcd9e93ca","permalink":"https://matfontaine.github.io/publication/yoshii-fast-nodate/","publishdate":"2020-12-23T09:33:25.353047Z","relpermalink":"/publication/yoshii-fast-nodate/","section":"publication","summary":"","tags":[],"title":"Fast Multichannel Correlated Tensor Factorization for Blind Source Separation","type":"publication"},{"authors":["Yicheng Du","Kouhei Sekiguchi","Yoshiaki Bando","Aditya Arie Nugraha","Mathieu Fontaine","Kazuyoshi Yoshii","Tatsuya Kawahara"],"categories":[],"content":"","date":1608716005,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608716005,"objectID":"342be1092ed4e419b7e846b74a7fcbe9","permalink":"https://matfontaine.github.io/publication/du-semi-supervised-nodate/","publishdate":"2020-12-23T09:33:25.514996Z","relpermalink":"/publication/du-semi-supervised-nodate/","section":"publication","summary":"","tags":[],"title":"Semi-supervised Multichannel Speech Separation Based on a Phone-and Speaker-Aware Deep Generative Model of Speech Spectrograms","type":"publication"},{"authors":[],"categories":[],"content":"","date":1608716003,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608716004,"objectID":"5c6b595979e8e523479ada9586056255","permalink":"https://matfontaine.github.io/publication/noauthor-zotero-nodate/","publishdate":"2020-12-23T09:33:23.930847Z","relpermalink":"/publication/noauthor-zotero-nodate/","section":"publication","summary":"","tags":[],"title":"Zotero textbar Downloads","type":"publication"},{"authors":["Prannay Khosla","Piotr Teterwak","Chen Wang","Aaron Sarna","Yonglong Tian","Phillip Isola","Aaron Maschinot","Ce Liu","Dilip Krishnan"],"categories":[],"content":"","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608716005,"objectID":"0dfdca6e0f8464a31653874c95e771fd","permalink":"https://matfontaine.github.io/publication/khosla-supervised-2020/","publishdate":"2020-12-23T09:33:25.663728Z","relpermalink":"/publication/khosla-supervised-2020/","section":"publication","summary":"Contrastive learning applied to self-supervised representation learning has seen a resurgence in recent years, leading to state of the art performance in the unsupervised training of deep image models. Modern batch contrastive approaches subsume or significantly outperform traditional contrastive losses such as triplet, max-margin and the N-pairs loss. In this work, we extend the self-supervised batch contrastive approach to the fully-supervised setting, allowing us to effectively leverage label information. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. We analyze two possible versions of the supervised contrastive (SupCon) loss, identifying the best-performing formulation of the loss. On ResNet-200, we achieve top-1 accuracy of 81.4% on the ImageNet dataset, which is 0.8% above the best number reported for this architecture. We show consistent outperformance over cross-entropy on other datasets and two ResNet variants. The loss shows benefits for robustness to natural corruptions and is more stable to hyperparameter settings such as optimizers and data augmentations. In reduced data settings, it outperforms cross-entropy significantly. Our loss function is simple to implement, and reference TensorFlow code is released at https://t.ly/supcon.","tags":["\"Computer Science - Computer Vision and Pattern Recognition\"","\"Computer Science - Machine Learning\"","\"Statistics - Machine Learning\""],"title":"Supervised Contrastive Learning","type":"publication"},{"authors":["Mathieu Fontaine","Roland Badeau","Antoine Liutkus"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608716004,"objectID":"c107f69bfdf7c890f7aee8d5c42798a2","permalink":"https://matfontaine.github.io/publication/fontaine-separation-2020/","publishdate":"2020-12-23T09:33:24.735473Z","relpermalink":"/publication/fontaine-separation-2020/","section":"publication","summary":"","tags":[],"title":"Separation of alpha-stable random vectors","type":"publication"},{"authors":["Mathieu Fontaine","Kouhei Sekiguchi","Aditya Arie Nugraha","Kazuyoshi Yoshii"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608716005,"objectID":"6733284b4f591c728103ec9ab5973af4","permalink":"https://matfontaine.github.io/publication/fontaine-unsupervised-2020/","publishdate":"2020-12-23T09:33:24.890426Z","relpermalink":"/publication/fontaine-unsupervised-2020/","section":"publication","summary":"","tags":[],"title":"Unsupervised Robust Speech Enhancement Based on Alpha-Stable Fast Multichannel Nonnegative Matrix Factorization","type":"publication"},{"authors":["Felix Putze","Dennis Weib","Lisa-Marie Vortmann","Tanja Schultz"],"categories":[],"content":"","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608716005,"objectID":"97459dc5f48c0940c393e68c8891ad73","permalink":"https://matfontaine.github.io/publication/putze-augmented-2019/","publishdate":"2020-12-23T09:33:25.820545Z","relpermalink":"/publication/putze-augmented-2019/","section":"publication","summary":"In this paper, we investigate the integration of eyetracking and a Brain-Computer Interface into an Augmented Reality system to control a smart home environment. Through a head-mounted display, we present context-dependent control elements which the user selects by directing attention towards them. We show that the combination of both modalities leads to the most robust detection of selections and an interface which is accepted by its users.","tags":[],"title":"Augmented Reality Interface for Smart Home Control using SSVEP-BCI and Eye Gaze","type":"publication"},{"authors":["Mathieu Fontaine","Aditya Arie Nugraha","Roland Badeau","Kazuyoshi Yoshii","Antoine Liutkus"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608716005,"objectID":"68cf36e3cfa8bda3507e1d8c1e9b1503","permalink":"https://matfontaine.github.io/publication/fontaine-cauchy-2019/","publishdate":"2020-12-23T09:33:25.200726Z","relpermalink":"/publication/fontaine-cauchy-2019/","section":"publication","summary":"","tags":[],"title":"Cauchy multichannel speech enhancement with a deep speech prior","type":"publication"},{"authors":["Mathieu Fontaine"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608716004,"objectID":"a27276c91a05e7fd2b55490b72a70498","permalink":"https://matfontaine.github.io/publication/fontaine-processus-2019/","publishdate":"2020-12-23T09:33:24.109935Z","relpermalink":"/publication/fontaine-processus-2019/","section":"publication","summary":"","tags":[],"title":"Processus alpha-stables pour le traitement du signal","type":"publication"},{"authors":["Mathieu Fontaine","Fabian-Robert Stöter","Antoine Liutkus","Umut Şimşekli","Romain Serizel","Roland Badeau"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608716005,"objectID":"6ebc84e9ccaf85abd79d44be2580fb05","permalink":"https://matfontaine.github.io/publication/fontaine-multichannel-2018/","publishdate":"2020-12-23T09:33:25.046759Z","relpermalink":"/publication/fontaine-multichannel-2018/","section":"publication","summary":"","tags":[],"title":"Multichannel audio modeling with elliptically stable tensor decomposition","type":"publication"},{"authors":["Mathieu Fontaine","Antoine Liutkus","Laurent Girin","Roland Badeau"],"categories":[],"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608716004,"objectID":"eb275a0089306990780983585e233d6b","permalink":"https://matfontaine.github.io/publication/fontaine-explaining-2017/","publishdate":"2020-12-23T09:33:24.582619Z","relpermalink":"/publication/fontaine-explaining-2017/","section":"publication","summary":"","tags":[],"title":"Explaining the parameterized Wiener filter with alpha-stable processes","type":"publication"},{"authors":["Mathieu Fontaine","Charles Vanwynsberghe","Antoine Liutkus","Roland Badeau"],"categories":[],"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608716004,"objectID":"a9436c86a5caedc8d05af0658fb78dfd","permalink":"https://matfontaine.github.io/publication/fontaine-scalable-2017/","publishdate":"2020-12-23T09:33:24.266146Z","relpermalink":"/publication/fontaine-scalable-2017/","section":"publication","summary":"","tags":[],"title":"Scalable source localization with multichannel α-stable distributions","type":"publication"},{"authors":["Mathieu Fontaine","Charles Vanwynsberghe","Antoine Liutkus","Roland Badeau"],"categories":[],"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608716004,"objectID":"876935694b5b3c2f6ce32fc84705fabd","permalink":"https://matfontaine.github.io/publication/fontaine-sketching-2017/","publishdate":"2020-12-23T09:33:24.426891Z","relpermalink":"/publication/fontaine-sketching-2017/","section":"publication","summary":"","tags":[],"title":"Sketching for nearfield acoustic imaging of heavy-tailed sources","type":"publication"},{"authors":["EAv Hammerstein"],"categories":[],"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608716006,"objectID":"a1409c64baea81d9f481d24132e7c3c5","permalink":"https://matfontaine.github.io/publication/hammerstein-generalized-2010/","publishdate":"2020-12-23T09:33:26.294959Z","relpermalink":"/publication/hammerstein-generalized-2010/","section":"publication","summary":"","tags":[],"title":"Generalized hyperbolic distributions: theory and applications to CDO pricing","type":"publication"}]